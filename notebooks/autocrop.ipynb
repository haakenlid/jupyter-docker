{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Face and feature detection with opencv. \"\"\"\n",
    "\n",
    "import os\n",
    "from collections import namedtuple\n",
    "\n",
    "import cv2\n",
    "import numpy\n",
    "\n",
    "Cropping = namedtuple('Cropping', ['top', 'left', 'diameter'])\n",
    "\n",
    "CASCADE_FILE = os.path.join(\n",
    "    '/usr/local/share/OpenCV/haarcascades/',\n",
    "    'haarcascade_frontalface_default.xml'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def autocrop(instance):\n",
    "    \"\"\" Try different algorithms, change crop and save model. \"\"\"\n",
    "    try:\n",
    "        grayscale_image = cv2.cvtColor(\n",
    "            opencv_image(instance), cv2.COLOR_RGB2GRAY)\n",
    "    except AttributeError as err:  # No file access?\n",
    "        logger.exception('Autocrop failed')\n",
    "        return\n",
    "    cropping, faces = detect_faces(grayscale_image)\n",
    "    if faces == 1:\n",
    "        cropping_method = instance.CROP_PORTRAIT\n",
    "    elif faces > 1:\n",
    "        cropping_method = instance.CROP_FACES\n",
    "    else:\n",
    "        cropping = detect_features(grayscale_image)\n",
    "        cropping_method = instance.CROP_FEATURES\n",
    "\n",
    "    left = int(round(100 * cropping.left /\n",
    "                     grayscale_image.shape[1]))\n",
    "    top = int(round(100 * cropping.top /\n",
    "                    grayscale_image.shape[0]))\n",
    "    diameter = int(round(100 * cropping.diameter /\n",
    "                         min(grayscale_image.shape)))\n",
    "\n",
    "    del(grayscale_image)  # Might save some memory?\n",
    "\n",
    "    diameter = min(100, diameter)\n",
    "\n",
    "    return left, top, diameter, cropping_method\n",
    "\n",
    "\n",
    "def detect_faces(cv2img,):\n",
    "    \"\"\" Detects faces in image and adjust cropping. \"\"\"\n",
    "    # http://docs.opencv.org/trunk/modules/objdetect/doc/cascade_classification.html\n",
    "    # cv2.CascadeClassifier.detectMultiScale(image[, scaleFactor[,\n",
    "    # minNeighbors[, flags[, minSize[, maxSize]]]]])\n",
    "    # cascade – Haar classifier cascade (OpenCV 1.x API only). It can\n",
    "    # be loaded from XML or YAML file using Load(). When the cascade is\n",
    "    # not needed anymore, release it using\n",
    "    # cvReleaseHaarClassifierCascade(&cascade).\n",
    "    # image – Matrix of the type CV_8U containing an image where\n",
    "    # objects are detected.\n",
    "    # objects – Vector of rectangles where each rectangle contains the\n",
    "    # detected object, the rectangles may be partially outside the\n",
    "    # original image.\n",
    "    # numDetections – Vector of detection numbers for the corresponding\n",
    "    # objects. An object’s number of detections is the number of\n",
    "    # neighboring positively classified rectangles that were joined\n",
    "    # together to form the object.\n",
    "    # scaleFactor – Parameter specifying how much the image size is\n",
    "    # reduced at each image scale.\n",
    "    # minNeighbors – Parameter specifying how many neighbors each\n",
    "    # candidate rectangle should have to retain it.\n",
    "    # flags – Parameter with the same meaning for an old cascade as in\n",
    "    # the function cvHaarDetectObjects. It is not used for a new\n",
    "    # cascade.\n",
    "    # minSize – Minimum possible object size. Objects smaller than that\n",
    "    # are ignored.\n",
    "    # maxSize – Maximum possible object size. Objects larger than that\n",
    "    # are ignored.\n",
    "    # outputRejectLevels – Boolean. If True, it returns the\n",
    "    # rejectLevels and levelWeights. Default value is False.\n",
    "\n",
    "    face_cascade = cv2.CascadeClassifier(CASCADE_FILE)\n",
    "    # faces = face_cascade.detectMultiScale(cv2img,)\n",
    "    faces = face_cascade.detectMultiScale(\n",
    "        cv2img, minSize=(50, 50), minNeighbors=10)\n",
    "\n",
    "    # faces = sorted(faces, reverse=True, key=lambda l, t, w, h:\n",
    "    # w*h)[:max_matches]\n",
    "    horizontal_faces, vertical_faces = [], []\n",
    "    box = {}\n",
    "    for (face_left, face_top, face_width, face_height) in faces:\n",
    "        # Create weighted average of faces. Bigger is heavier.\n",
    "        horizontal_faces.extend(\n",
    "            [face_left + face_width / 2] * face_width)\n",
    "        vertical_faces.extend(\n",
    "            [face_top + face_height / 2] * face_height)\n",
    "\n",
    "        face_right = face_left + face_width\n",
    "        face_bottom = face_top + face_height\n",
    "        box['left'] = min(\n",
    "            face_left, box.get('left', face_left))\n",
    "        box['top'] = min(\n",
    "            face_top, box.get('top', face_top))\n",
    "        box['right'] = max(\n",
    "            face_right, box.get('right', face_right))\n",
    "        box['bottom'] = max(\n",
    "            face_bottom, box.get('bottom', face_bottom))\n",
    "\n",
    "    if horizontal_faces:\n",
    "        left = sum(horizontal_faces) / len(horizontal_faces)\n",
    "        top = sum(vertical_faces) / len(vertical_faces)\n",
    "        diameter = max(\n",
    "            box['right'] - box['left'],\n",
    "            box['bottom'] - box['top']\n",
    "        )\n",
    "        return Cropping(\n",
    "            left=left, top=top, diameter=diameter), len(faces)\n",
    "    else:\n",
    "        # No faces found\n",
    "        return None, len(faces)\n",
    "\n",
    "\n",
    "def detect_features(cv2img):\n",
    "    \"\"\" Detect features in the image to determine cropping \"\"\"\n",
    "    # http://docs.opencv.org/trunk/modules/imgproc/doc/feature_detection.html\n",
    "    # cv2.goodFeaturesToTrack(image, maxCorners, qualityLevel,\n",
    "    # minDistance[, corners[, mask[, blockSize[, useHarrisDetector[,\n",
    "    # k]]]]]) → corners\n",
    "    # image – Input 8-bit or floating-point 32-bit, single-channel\n",
    "    # image.\n",
    "    # corners – Output vector of detected corners.\n",
    "    # maxCorners – Maximum number of corners to return. If there are\n",
    "    # more corners than are found, the strongest of them is returned.\n",
    "    # qualityLevel – Parameter characterizing the minimal accepted\n",
    "    # quality of image corners. The parameter value is multiplied by\n",
    "    # the best corner quality measure, which is the minimal eigenvalue\n",
    "    # (see cornerMinEigenVal() ) or the Harris function response (see\n",
    "    # cornerHarris() ). The corners with the quality measure less than\n",
    "    # the product are rejected. For example, if the best corner has the\n",
    "    # quality measure = 1500, and the qualityLevel=0.01 , then all the\n",
    "    # corners with the quality measure less than 15 are rejected.\n",
    "    # minDistance – Minimum possible Euclidean distance between the\n",
    "    # returned corners.\n",
    "    # mask – Optional region of interest. If the image is not empty (it\n",
    "    # needs to have the type CV_8UC1 and the same size as image ), it\n",
    "    # specifies the region in which the corners are detected.\n",
    "    # blockSize – Size of an average block for computing a derivative\n",
    "    # covariation matrix over each pixel neighborhood. See\n",
    "    # cornerEigenValsAndVecs() .\n",
    "    # useHarrisDetector – Parameter indicating whether to use a Harris\n",
    "    # detector (see cornerHarris()) or cornerMinEigenVal().\n",
    "    # k – Free parameter of the Harris detector.\n",
    "\n",
    "    corners = cv2.goodFeaturesToTrack(cv2img, 25, 0.01, 10)\n",
    "    corners = numpy.int0(corners)\n",
    "    xx = corners.ravel()[0::2]\n",
    "    yy = corners.ravel()[1::2]\n",
    "    w = max(xx) - min(xx)\n",
    "    h = max(yy) - min(yy)\n",
    "    d = max(w, h)\n",
    "    x = xx.mean()\n",
    "    y = yy.mean()\n",
    "    return Cropping(left=x, top=y, diameter=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def opencv_image(image_file, size=400):\n",
    "    \"\"\" Convert ImageFile into a cv2 image for image processing. \"\"\"\n",
    "\n",
    "    with open(image_file, 'rb') as img:\n",
    "        nparr = numpy.fromstring(img.read(), numpy.uint8)\n",
    "\n",
    "    cv2img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
    "    cv2img = cv2.cvtColor(cv2img, cv2.COLOR_BGR2RGB)\n",
    "    width, height = cv2img.shape[0], cv2img.shape[1]\n",
    "    if width > height:\n",
    "        height, width = size * height // width, size\n",
    "    else:\n",
    "        height, width = size, size * width // height\n",
    "\n",
    "    cv2img = cv2.resize(cv2img, (height, width))\n",
    "    cv2img = cv2.cvtColor(cv2img, cv2.COLOR_RGB2GRAY)\n",
    "    return cv2img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2017-01-25 15:49:27--  http://static.universitas.no/media/thumb-cache/77/2d/772dd8238e7bad07c27dca6612f5ec5f/02-NYH-underviser-AN-1.jpg\n",
      "Resolving static.universitas.no (static.universitas.no)... 52.219.73.23\n",
      "Connecting to static.universitas.no (static.universitas.no)|52.219.73.23|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 111937 (109K) [image/jpeg]\n",
      "Saving to: ‘02-NYH-underviser-AN-1.jpg.2’\n",
      "\n",
      "02-NYH-underviser-A 100%[=====================>] 109.31K   644KB/s   in 0.2s   \n",
      "\n",
      "2017-01-25 15:49:28 (644 KB/s) - ‘02-NYH-underviser-AN-1.jpg.2’ saved [111937/111937]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://static.universitas.no/media/thumb-cache/77/2d/772dd8238e7bad07c27dca6612f5ec5f/02-NYH-underviser-AN-1.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Cropping(top=59.5, left=465.5, diameter=67), 1)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvimg = opencv_image('./02-NYH-underviser-AN-1.jpg', 1000)\n",
    "detect_faces(cvimg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cropping(top=224.75999999999999, left=235.0, diameter=507)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detect_features(cvimg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
